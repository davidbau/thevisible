<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
    
<script defer language="javascript" type="text/javascript" src="https://thevisible.net/js/light_dark.js"></script>
<script defer language="javascript" type="text/javascript" src="https://thevisible.net/js/tabs.js"></script>


<script defer language="javascript" type="text/javascript" src="https://thevisible.net/js/katex.min.js"></script>
<script defer language="javascript" type="text/javascript" src="https://thevisible.net/js/auto-render.min.js"></script>





<script defer language="javascript" type="text/javascript" src="https://thevisible.net/js/katex.js"></script>
<script defer language="javascript" type="text/javascript" src="https://thevisible.net/js/toc.js"></script>






<script async src="https://www.googletagmanager.com/gtag/js?id=G-FD12LWN557"></script>
<script>
   window.dataLayer = window.dataLayer || [];
   function gtag(){dataLayer.push(arguments);}
   gtag('js', new Date()); gtag('config', 'G-FD12LWN557');
</script>



    
    <meta http-equiv="content-type" content="text/html; charset=utf-8">

    
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    
    





  
 

 


  
  
  
 



  
  
    
  
  




<title itemprop="name">Is Artificial Intelligence Intelligent? - The Visible Net</title>
<meta property="og:title" content=Is&#32;Artificial&#32;Intelligence&#32;Intelligent?&#32;-&#32;The&#32;Visible&#32;Net />
<meta name="twitter:title" content=Is&#32;Artificial&#32;Intelligence&#32;Intelligent?&#32;-&#32;The&#32;Visible&#32;Net />
<meta itemprop="name" content=Is&#32;Artificial&#32;Intelligence&#32;Intelligent?&#32;-&#32;The&#32;Visible&#32;Net />
<meta name="application-name" content=Is&#32;Artificial&#32;Intelligence&#32;Intelligent?&#32;-&#32;The&#32;Visible&#32;Net />
<meta property="og:site_name" content="The Visible Net" />


<meta name="description" content="" />
<meta itemprop="description" content="" />
<meta property="og:description" content="" />
<meta name="twitter:description" content="" />


<base href="https://thevisible.net/posts/002-is-ai-intelligent/" />
<link rel="canonical" href="https://thevisible.net/posts/002-is-ai-intelligent/" itemprop="url" />
<meta name="url" content="https://thevisible.net/posts/002-is-ai-intelligent/" />
<meta name="twitter:url" content="https://thevisible.net/posts/002-is-ai-intelligent/" />
<meta property="og:url" content="https://thevisible.net/posts/002-is-ai-intelligent/" />





<meta property="og:updated_time" content=2004-02-05T430:38:23-0400 />


<link rel="sitemap" type="application/xml" title="Sitemap" href='https://thevisible.net/sitemap.xml' />

<meta name="robots" content="index,follow" />
<meta name="googlebot" content="index,follow" />


<meta name="twitter:site" content="" />
<meta name="twitter:creator" content="" />
<meta property="fb:admins" content="" />


<meta name="apple-mobile-web-app-title" content="The Visible Net" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black" />





 


<meta property="og:type" content="article" />
<meta property="article:publisher" content="" />
<meta property="og:article:published_time" content=2004-02-05T430:38:23-0400 />
<meta property="article:published_time" content=2004-02-05T430:38:23-0400 />





<script defer type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "Article",
    "headline": "Is Artificial Intelligence Intelligent?",
    "author": {
      "@type": "Person",
      "name": ""
    },
    "datePublished": "2023-04-02",
    "description": "",
    "wordCount":  2545 ,
    "mainEntityOfPage": "True",
    "dateModified": "2023-04-02",
    "image": {
      "@type": "imageObject",
      "url": ""
    },
    "publisher": {
      "@type": "Organization",
      "name": "The Visible Net",
      "logo": {
        "@type": "imageObject",
        "url": ""
      }
    }
  }
</script>



<meta name="generator" content="Hugo 0.111.3">


    
    <link type="text/css" rel="stylesheet" href="https://thevisible.net/css/poole.css">
<link type="text/css" rel="stylesheet" href="https://thevisible.net/css/syntax.css">
<link type="text/css" rel="stylesheet" href="https://thevisible.net/css/hyde.css">
<link type="text/css" rel="stylesheet" href="https://thevisible.net/css/poison.css">
<link type="text/css" rel="stylesheet" href="https://thevisible.net/css/fonts.css">
<link type="text/css" rel="stylesheet" href="https://thevisible.net/css/katex.min.css">
<link type="text/css" rel="stylesheet" href="https://thevisible.net/css/tabs.css">


<link type="text/css" rel="stylesheet" href="https://thevisible.net/css/custom.css">


    
    <style>
    body {
        --sidebar-bg-color: #EEE;
        --sidebar-img-border-color: #CCC;
        --sidebar-p-color: #000;
        --sidebar-h1-color: #000;
        --sidebar-a-color: #000;
        --sidebar-socials-color: #000;
        --text-color: #222;
        --bkg-color: #FFF;
        --post-title-color: #303030;
        --list-color: #5a5a5a;
        --link-color: #268bd2;
        --date-color: #515151;
        --table-border-color: #E5E5E5;
        --table-stripe-color: #F9F9F9;
        --code-color: #bf616a;
        --code-background-color: #E5E5E5;
        --moon-sun-color: #EEE;
        --moon-sun-background-color: #CCC;
    }
    body.dark-theme {
        --text-color: #eee;
        --bkg-color: #121212;
        --post-title-color: #DBE2E9;
        --list-color: #9d9d9d;
        --link-color: #268bd2;
        --date-color: #9a9a9a;
        --table-border-color: #515151;
        --table-stripe-color: #202020;
        --code-color: #ff7f7f;
        --code-background-color: #393D47;
    }
    body {
        background-color: var(--bkg-color);
    }
</style>

</head>

    <body class="">
        <aside class="sidebar">
    <div class="container sidebar-sticky">
        <div class="light-dark" align="right">
    <button class="btn-light-dark">
        <svg class="moon" xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" viewBox="0 0 16 16">
            <path fill="currentColor" d="M6 .278a.768.768 0 0 1 .08.858a7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277c.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316a.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71C0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"/>
        </svg>
        <svg class="sun" xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" viewBox="0 0 16 16">
            <path fill="currentColor" d="M8 12a4 4 0 1 0 0-8a4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"/>
        </svg>
    </button>
</div>

        <div class="sidebar-about">
    <h1 class="brand">
        
            <a href="https://thevisible.net/">
                <img src="/images/side_bulb_square.jpg" alt="brand image">
            </a>
        
        
            <a href="https://thevisible.net/">
                <h1>The Visible Net</h1>
            </a>
        
    </h1>
    <p class="lead">
    Science, Society and Understanding AI
    </p>
</div>

        <nav>
    <ul class="sidebar-nav">

        
        
        
        
            
                
                    <li class="heading">
                        <a href="/about/">About</a>
                    </li>
                    
                
            
                
            
            
                
            
                
            
        
        
            
                
            
                
                    <li class="heading">
                        <a href="/posts/">Posts</a>
                    </li>
                    
                        <li class="sub-heading">
                            Recent
                        </li>
                        
                            <li class="bullet">
                                <a href="https://thevisible.net/posts/003-national-deep-inference-facility/">A National Deep Inference Facility</a>
                            </li>
                        
                            <li class="bullet">
                                <a href="https://thevisible.net/posts/002-is-ai-intelligent/">Is Artificial Intelligence Intelligent?</a>
                            </li>
                        
                            <li class="bullet">
                                <a href="https://thevisible.net/posts/001-catching-up/">Catching Up</a>
                            </li>
                        
                            <li class="bullet">
                                <a href="https://thevisible.net/posts/000-welcome/">Welcome</a>
                            </li>
                        
                    
                
            
            
                
            
                
            
        

    </ul>
</nav>

        












        <p class="footnote">

    <br>
    &copy; 2025 The Visible Net. All rights reserved.
</p>

  </div>
</aside>

        <main class="content container">
            <div class="post">
  <div class="info">
    <h1 class="post-title">
        <a href="https://thevisible.net/posts/002-is-ai-intelligent/">Is Artificial Intelligence Intelligent?</a>
    </h1>
    <time datetime="2023-04-02T17:30:38-0400" class="post-date">
        April 2, 2023
    </time>
    
    &nbsp;
    <address class="post-date">
        David Bau
    </address>
    

    
    
    
    
    
    
</div>

  <p>The idea that large language models could be capable of cognition is not obvious. Neural language modeling has been around since <a href="https://papers.baulab.info/Elman-1990.pdf">Jeff Elman’s 1990 structure-in-time work</a>, but 33 years passed between that initial idea and <a href="https://papers.baulab.info/also/Bubek-2023.pdf">first contact with ChatGPT</a>.</p>
<p>What took so long?  In this blog I write about why few saw it coming, why some remain skeptical even in the face of amazing GPT-4 behavior, why machine cognition may be emerging anyway, and what we should study next.</p>
<h1 id="spark-joness-worry-about-language-modeling">Spark-Jones’s worry about language modeling</h1>
<p>This blog was inspired by <a href="https://papers.baulab.info/also/Spark-Jones-2004.pdf">Karen Spark-Jones’ 2004 note</a> that asks whether the generative model for language modeling is rational or not. In it, she points out that language models (such as GPT) are based on a highly implausible statistical model of the mechanisms of language, like the one depicted here:</p>
<p><img src="/images/graph_wordpred.png" alt="Simple autoregressive graphical model"></p>
<p>The reason this picture seems so unlikely to lead to a rational model of intelligence is that <em>nobody actually believes that words cause other words!</em> This graphical model is just a shorthand for expressing the assertion that the probability distribution of y depends on nothing else but the observation of the previous words x. When critics note that LLMs are mere <a href="https://dl.acm.org/doi/pdf/10.1145/3442188.3445922">stochastic parrots</a>, warning of the <a href="https://www.nytimes.com/2023/03/08/opinion/noam-chomsky-chatgpt-ai.html">false promise of ChatGPT</a>, the implausibility of this picture is at the root of their argument.  Words do not spring from other words. But that is the basic assumption that language models make.</p>
<p>Consider the example of the <a href="https://en.wikipedia.org/wiki/Loebner_Prize">Turing-test-contestant</a> programs such as <a href="https://en.wikipedia.org/wiki/ELIZA">ELIZA</a>, <a href="https://en.wikipedia.org/wiki/Artificial_Linguistic_Internet_Computer_Entity">ALICE</a>, <a href="https://en.wikipedia.org/wiki/Jabberwacky">Jabberwacky</a>, <a href="https://en.wikipedia.org/wiki/Cleverbot">Cleverbot</a> and <a href="https://en.wikipedia.org/wiki/Eugene_Goostman">Eugene Goostman</a>.  These are structured like the graphical model above, imitating human conversation by choosing textual responses based on statistics and pattern-matching on previous words. Nobody, not even the creators of those systems, seriously believes that the design of such pattern-matching engines contains profound cognitive capabilities. They are parlor tricks, automata that display a shallow semblance of intelligence while just following simple procedures.</p>
<p>Yet, somehow, though <a href="https://chat.openai.com/">ChatGPT</a> works the same way, it seems more profound.  What is the difference?  Is ChatGPT the same as Jabberwaacky; separated only by scale, a few years of Moore&rsquo;s law, a big budget, and slightly better programming?  Or if they are substantively different, what is the fundamental difference?  What line has been crossed?</p>
<p>To appreciate what Spark-Jones found to be missing in the traditional language modeling view, contrast the diagram above to the following graph that provides a more rational model for the cognitive process of language.</p>
<p><img src="/images/graph_meaning1.png" alt="Simple graphical model incorporating meaning"></p>
<p>Spark-Jones drew graphs like this to indicate what we are really after. Here the “m” denotes the underlying meaning that exists within your mind, causing you to utter the words &ldquo;Shaquille O&rsquo;Neal plays basketball.&rdquo; This model is more plausible, because it is not words that generate other words, rather, it is our thoughts, knowledge, intentions and <em>desire for expression</em> that cause words to be spoken.</p>
<h1 id="gans-have-the-right-shape-but-cannot-do-language">GANs have the right shape but cannot do language</h1>
<p>Before you object that such an imaginative abstraction is unrelated to practical considerations in the field of artificial neural networks, keep in mind that it is common to create neural architectures with an explicit state vector that plays the role of m.  For example, contrast <a href="https://arxiv.org/abs/1606.05328">Pixel-CNN networks</a>, which model an image by predicting each pixel as a consequence of previously-seen pixels above and to the left, with <a href="https://arxiv.org/abs/1406.2661">generative adversarial networks (GANs)</a>, which use an explicit representation stored in a small hidden state z that predicts all the pixels, with no upstream dependencies.</p>
<p>Both architectures are able to synthesize realistic-looking images of the world. However, it seems very unlikely that the Pixel-CNN architecture would contain any sensible representation of the world, because <em>nobody believes that pixels cause other pixels.</em></p>
<p><img src="/images/graph_pixelcnn_vs_gan.png" alt="PixelCNN vs GAN models"></p>
<p>On the other hand, the GAN architecture seems more promising, because it posits a set of variables z that are the cause of all the pixels.  In a GAN, we are hoping for z to represent “state of the world” and “state of the camera,” and for this state to lead to a reasonable set of calculations that produce the image of a realistic scene.</p>
<p>Remarkably, with this setup, GANs show evidence of learning rational models of the world.  If you are unfamiliar with GANs, I recommend reading <a href="https://arxiv.org/abs/1812.04948">Karras’s StyleGAN papers</a> and then the <a href="https://arxiv.org/abs/2011.12799">StyleSpace paper from Wu</a>. Wu found that there is a small set of bottleneck “stylespace” neurons within StyleGAN that correspond to real-world concepts such as whether a person is wearing glasses or whether they are smiling.  The results are empirical, but the phenomenon has been <a href="https://gandissect.csail.mit.edu/">observed in various GAN models</a> trained on different data sets.</p>
<p>When we reproduce Wu&rsquo;s results on a GAN trained to draw bedrooms, we find a single individual neuron that controls whether the lights are turned on or off.  There was no explicit supervision that led to this disentangled neuron being learned: the GAN was just trained on individual images and never saw a video where a light was switched.  But for some reason, the learned model arrived at a solution where there is a distinctive &ldquo;light switch&rdquo; neuron that imitates the action of a light switch in the real world.</p>
<p><img src="/images/gan_neuron_control.gif" alt="GAN single-neuron control"></p>
<p>So rational graphical models seem to be a promising path for possibly learning reasonable models of the world, maybe even reasonable models of cognition.  Unfortunately, GANs and similar architectures have not (yet) been successful at modeling anything as complex as natural language.  That might be because the cognitive processes that lead to language are too intricate for these architectures. Ultimately, humans draw upon an enormous amount of knowledge just to conjure up a simple utterance.</p>
<h1 id="transformers-have-the-wrong-shape-but-may-harbor-knowledge">Transformers have the wrong shape but may harbor knowledge</h1>
<p>Consider the sentence &ldquo;Shaquille O&rsquo;Neal plays basketball.&rdquo; Although the words “soccer” or “tennis” are often reasonable alternatives to the word “basketball,” when talking about Shaq playing basketball, other sports cannot be substituted in that particular sentence.</p>
<p>This is because the sentence is not just making a grammatically correct statement.  The sentence reflects a humanlike thinking process: it reflects our knowledge about Shaq. We would only say “soccer” if we meant the sport of a soccer player like Megan Rapinoe, or if&mdash;for some reason&mdash;we pretended to hold the mistaken belief that Shaq played soccer. A decomposition of the individual ideas within our mind might be diagrammed like the figure on the left.</p>
<p><img src="/images/graph_side_by_side.png" alt="Simple graphical model decomposing knowledge"></p>
<p>Unfortunately, autoregressive models have the &ldquo;wrong&rdquo; top-level structure to directly implement the model on the left: in an autoregressive transformer, preceding words are inputs rather than outputs.</p>
<p>On the other hand, research from my lab by Kevin Meng et al. (<a href="https://rome.baulab.info">ROME</a>)
suggests that transformers can implement reasonable models of cognition by learning explicit hidden states that carry information about a meaningful facet of the world.  The graphs end up looking like the picture on the right, with the same structure as a rational model with some arrows reversed.</p>
<p>Consider the plot below, which shows the effect of swapping individual hidden states between two runs of a GPT model, one with a sentence about Shaquille O&rsquo;Neal, and another with a sentence about Megan Rapinoe.</p>
<p>Ordinarily the sentence is about Shaq and predicts &ldquo;basketball.&rdquo;  But if we take a single hidden state from Megan&rsquo;s sentence and move it over, then at some locations (shown in purple), it will cause the model to flip its predictions to &ldquo;soccer.&rdquo; It is unsurprising that swapping states late in the model, at (b) will cause this effect, but the surprising finding is that a small set of states deep within the model, at (a), also cause the model to flip its predictions.</p>
<p>In the ROME paper we find evidence that these early states correspond to the point at which the model retrieves its knowledge about which sport the athlete plays.  For example, if we intercept and modify the parameters of the model in the early site (a), we can edit the model&rsquo;s belief and make it think that Shaq plays soccer.</p>
<p><img src="/images/rome_rapinoe_shaq.png" alt="Switching Shaq to a soccer player"></p>
<p>Kevin&rsquo;s interpretation of the causal structure within the network is pretty effective.  In his <a href="https://memit.baulab.info/">MEMIT paper</a>, he finds that he can use his understanding of the structure to directly edit transformer memories at scale, providing control over transformer memories that is several orders-of-magnitude better than traditional fine-tuning methods.</p>
<p>The emergence of reasonable &ldquo;world models&rdquo; inside large transformers, despite the reversed direction of the arrows, has been observed in several other works that are worth reading about.  Be sure to read the <a href="https://thegradient.pub/othello/">Othello</a> paper by Kenneth Li, et al., as well as <a href="https://www.neelnanda.io/mechanistic-interpretability/othello">Neel Nanda&rsquo;s followup</a>.  Read the <a href="https://arxiv.org/abs/2209.11895">induction heads</a> paper by Catherine Olssen, et al., the <a href="https://arxiv.org/abs/2106.00737">Alchemy</a> paper by Belinda Li, et al., and the <a href="https://arxiv.org/abs/2211.00593">indirect object identification</a> work by Kevin Wang, et al.  All these works reveal little pieces of a secret:</p>
<blockquote>
<p><em>Transformer models do not just imitate surface statistics.  They go beyond that, often constructing internal computational mechanisms that mimic causal mechanisms in the real world.</em></p>
</blockquote>
<h1 id="beyond-the-turing-test">Beyond the Turing Test</h1>
<p>A decade ago, <a href="https://www.npr.org/2014/06/14/322008378/moving-beyond-the-turing-test-to-judge-artificial-intelligence">Gary Marcus argued that we need to move beyond the Turing test</a> as our metric for the emergence of machine intelligence.  He observed that the test is too easy, that humans are too easily fooled by the mere appearance of intelligent behavior. He argued that a true intelligent agent would contain rational thoughts, that would allow them to understand actual relationships and motivations and causes in the world rather than mere word statistics.  At the time, <a href="https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/2650/2527">Marcus gathered together a series of more difficult tests</a> of behavior, such as open-ended questions about movies and stories, questions about theory of mind, motivation, desires, and problems that seem to demand an understanding of the world such as visual question answering.</p>
<p>Yet now a decade later, just when <a href="https://papers.baulab.info/also/Bubek-2023.pdf">large language models are beginning to solve all these more-difficult tasks</a>, Marcus continues to <a href="https://www.nytimes.com/2023/01/06/opinion/ezra-klein-podcast-gary-marcus.html">meet the results with skepticism</a>, observing that the massive scale of training data might still be fooling us.  He points at flaws in logic and knowledge as evidence that the models are not really thinking.</p>
<p>But there is an obvious gap in Marcus&rsquo;s current objections. Is perfect logic and complete knowledge a prerequisite for &ldquo;thought?&rdquo;  Certainly most of us human beings are not capable of total factual recall and flawless logical reasoning.  Today, Marcus&rsquo;s solution for the dilemma seems to fall short.</p>
<p>The apparent need for an endless escalation of more-difficult external tasks for probing cognition suggests that Turing&rsquo;s basic framework has been missing an essential point.  <a href="https://papers.baulab.info/also/Turing-1950.pdf">In 1950, Turing proposed that we do not care about the implementation of a machine intelligence</a>. However, today it is becoming increasingly clear that fooling an external judge is <em>not</em> enough.  We <em>do</em> care what is inside the box.</p>
<p>The mechanistic-interpretability research program shows a way out of this trap.  Instead of suggesting ever-more-difficult external tests of behavior, it aims to develop experimental methods that tear back the curtain on black-box-models, asking &ldquo;what does a model learn?&rdquo; by uncovering the forms of computation the model contains, and asking whether those computations capture useful, meaningful, and causal structure about the world.</p>
<p>We are in early days, and this new research program is very immature.  We do not yet have abstractions that describe what it is that we are doing in a satisfying way.  But as the world grapples with the emergence of surprising intelligent behavior in large models, the detailed study of mechanisms within those models will become increasingly important.  A study of machine-learned mechanisms offers a new path for understanding what our big computers are really learning.</p>
<hr>
<h1 id="footnotes-and-responses">Footnotes and responses</h1>
<p>Some other things worth reading that are related to the ideas here: <a href="https://www.inference.vc/we-may-be-surprised-again/">Ferenc Huszár wrote a very interesting contemplation of other reasons he missed the promise of LLMs</a>, including the No Free Lunch theorem, and his own reasoning about the futility of maximizing p(data).  On the optimistic side, <a href="https://arxiv.org/abs/2212.01681">Jacob Andreas&rsquo;s Language Models as Agent Models paper</a> hypothesizes that LM can model intention because they are models of the communicative intent of writers.  <a href="https://medium.com/@mijordan3/artificial-intelligence-the-revolution-hasnt-happened-yet-5e1d5812e1e7">Michael Jordan has a different take, arguing that the Turing test is no longer interesting</a> because we should be setting our sights higher than mere imitation of humans!</p>
<p>After posting the above, I received the following throughtful responses.</p>
<p><a href="https://t3.technion.ac.il/researcher/belinkov-yonatan/">Yonatan Belinkov</a> writes:</p>
<ol>
<li>Next word prediction may appear irrational, but we should remember that it is a way to get the probability of any body of text via the chain rule. <a href="https://twitter.com/ryandcotterell/status/1643306052203470848?s=20">Ryan Cotterell recently made a comment on twitter</a> reminding us that p(text) is a very strong thing. In a sense, maybe p(text) is all we care about, and next word prediction is just one way to get that p. It is also interesting to think about whether the autoregressive factorization is the preferred way for some reason.</li>
<li>We must acknowledge that ChatGPT and similar models do more than next word prediction. First, their pre-training data sets are not just ordinary natural language. They are trained on code, and that gives them possibly external semantics via assertions.  See <a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00412/107385/Provable-Limitations-of-Acquiring-Meaning-from">this theoretical paper in TACL by William Merrill et al.</a> Second, these models are fine-tuned on instruction data sets. Third, reinforcement learning from human feedback (RLHF) is applied.</li>
<li>We should also remember that there is a gap between linguistic skills and other non-language-specific cognitive capabilities that may be essential parts of human thought.  Read the <a href="https://arxiv.org/pdf/2301.06627.pdf">recent survey by Mahowald and Ivanova, et al.</a></li>
<li>I am a bit cautious of our heat maps from ROME’s causal tracing given both the <a href="https://arxiv.org/abs/2301.04213">paper from Hase, et al</a> as well as follow-on experiments from our own labs. I think the results are meaningful but perhaps the interpretation will ultimately be more subtle.</li>
</ol>
<p>David&rsquo;s responses to Yonatan:</p>
<ol>
<li>If there is any lesson from machine learning, it is that modeling p(data) is impressively strong as an objective; maybe it is the most complete way to express the Turing test. On the other hand, what I am suggesting is p(data) is just serving as a powerful pretext task. A rational <em>algorithm</em> implementing the model is the real goal, and that goal is not perfectly equivalent to matching external observations of p(data).</li>
<li>Yes, in this blog narrative, I have swept <a href="https://thevisible.net/posts/001-catching-up/">implementation details of ChatGPT under the rug</a>, and readers need to be aware that <a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf">instruction-tuning and RLHF are being used, e.g., read Christiano, et al.</a> To me, it seems that the language modeling pre-training is the “secret sauce” behind ChatGPT. Of course GPT-3+ models are closed and proprietary, so we can only guess, but it seems likely that the vast majority of training data and computation is spent on pre-training, and that the fine-tuning just imparts a final domain shift to prepare the model for dialogue.  Thanks for the link to Merrill about the possibly profound importance of pre-training on code.</li>
<li>Thanks for the link to Mahowald and Ivanova’s survey. They remind us that humility is definitely in order when comparing language models to the breadth of human cognitive capabilities. Nevertheless, I think we have all been impressed and surprised at how many seemingly <a href="https://papers.baulab.info/also/Bubek-2023.pdf">non-linguistic capabilities</a> have emerged from the language modeling task.</li>
<li>In the end, I think our ROME findings about the organization of factual recall in transformers will be found to be robust—some explorations of various ways to stress-test the results are already included in our ROME appendix <a href="https://arxiv.org/abs/2202.05262">on arxiv</a>, and of course the scalability of <a href="https://memit.baulab.info/">MEMIT</a> is a powerful validation—although I agree, the picture should be clarified further, and more experiments in more settings and with more models will be needed.</li>
</ol>

  <hr>
<div class="footer">
    
	    
            
	            <a class="previous-post" href="https://thevisible.net/posts/001-catching-up/?ref=footer">« Catching Up</a>
	        
        
	    
            
	            <a class="next-post" href="https://thevisible.net/posts/003-national-deep-inference-facility/?ref=footer">A National Deep Inference Facility »</a>
	        
        
    
</div>

  <div class="article-toc " >
    <h4></h4>
    <nav id="TableOfContents"></nav>
</div>

</div>
        </main>
    </body>
</html>
