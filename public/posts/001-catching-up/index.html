<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
    
<script defer language="javascript" type="text/javascript" src="/js/light_dark.js"></script>
<script defer language="javascript" type="text/javascript" src="/js/tabs.js"></script>


<script defer language="javascript" type="text/javascript" src="/js/katex.min.js"></script>
<script defer language="javascript" type="text/javascript" src="/js/auto-render.min.js"></script>





<script defer language="javascript" type="text/javascript" src="/js/katex.js"></script>
<script defer language="javascript" type="text/javascript" src="/js/toc.js"></script>





    
    <meta http-equiv="content-type" content="text/html; charset=utf-8">

    
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    
    





  
 

 


  
  
  
 



  
  
    
  
  




<title itemprop="name">Catching Up - </title>
<meta property="og:title" content=Catching&#32;Up&#32;-&#32; />
<meta name="twitter:title" content=Catching&#32;Up&#32;-&#32; />
<meta itemprop="name" content=Catching&#32;Up&#32;-&#32; />
<meta name="application-name" content=Catching&#32;Up&#32;-&#32; />
<meta property="og:site_name" content="" />


<meta name="description" content="" />
<meta itemprop="description" content="" />
<meta property="og:description" content="" />
<meta name="twitter:description" content="" />


<base href="/posts/001-catching-up/" />
<link rel="canonical" href="/posts/001-catching-up/" itemprop="url" />
<meta name="url" content="/posts/001-catching-up/" />
<meta name="twitter:url" content="/posts/001-catching-up/" />
<meta property="og:url" content="/posts/001-catching-up/" />





<meta property="og:updated_time" content=26003-26-02T336:12:23-0400 />


<link rel="sitemap" type="application/xml" title="Sitemap" href='/sitemap.xml' />

<meta name="robots" content="index,follow" />
<meta name="googlebot" content="index,follow" />


<meta name="twitter:site" content="https://twitter.com" />
<meta name="twitter:creator" content="https://twitter.com" />
<meta property="fb:admins" content="" />


<meta name="apple-mobile-web-app-title" content="" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black" />





 


<meta property="og:type" content="article" />
<meta property="article:publisher" content="https://facebook.com" />
<meta property="og:article:published_time" content=26003-26-02T336:12:23-0400 />
<meta property="article:published_time" content=26003-26-02T336:12:23-0400 />


  <meta property="og:article:author" content="David bau" />
  <meta property="article:author" content="David bau" />
  <meta name="author" content="David bau" />




<script defer type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "Article",
    "headline": "Catching Up",
    "author": {
      "@type": "Person",
      "name": "https:\/\/github.com"
    },
    "datePublished": "2023-03-26",
    "description": "",
    "wordCount":  2645 ,
    "mainEntityOfPage": "True",
    "dateModified": "2023-03-26",
    "image": {
      "@type": "imageObject",
      "url": ""
    },
    "publisher": {
      "@type": "Organization",
      "name": "",
      "logo": {
        "@type": "imageObject",
        "url": ""
      }
    }
  }
</script>



<meta name="generator" content="Hugo 0.111.3">


    
    <link type="text/css" rel="stylesheet" href="/css/poole.css">
<link type="text/css" rel="stylesheet" href="/css/syntax.css">
<link type="text/css" rel="stylesheet" href="/css/hyde.css">
<link type="text/css" rel="stylesheet" href="/css/poison.css">
<link type="text/css" rel="stylesheet" href="/css/fonts.css">
<link type="text/css" rel="stylesheet" href="/css/katex.min.css">
<link type="text/css" rel="stylesheet" href="/css/tabs.css">


<link type="text/css" rel="stylesheet" href="/css/custom.css">


    
    <style>
    body {
        --sidebar-bg-color: #202020;
        --sidebar-img-border-color: #515151;
        --sidebar-p-color: #909090;
        --sidebar-h1-color: #FFF;
        --sidebar-a-color: #FFF;
        --sidebar-socials-color: #FFF;
        --text-color: #222;
        --bkg-color: #FAF9F6;
        --post-title-color: #303030;
        --list-color: #5a5a5a;
        --link-color: #268bd2;
        --date-color: #515151;
        --table-border-color: #E5E5E5;
        --table-stripe-color: #F9F9F9;
        --code-color: #bf616a;
        --code-background-color: #E5E5E5;
        --moon-sun-color: #FFF;
        --moon-sun-background-color: #515151;
    }
    body.dark-theme {
        --text-color: #eee;
        --bkg-color: #121212;
        --post-title-color: #DBE2E9;
        --list-color: #9d9d9d;
        --link-color: #268bd2;
        --date-color: #9a9a9a;
        --table-border-color: #515151;
        --table-stripe-color: #202020;
        --code-color: #ff7f7f;
        --code-background-color: #393D47;
    }
    body {
        background-color: var(--bkg-color);
    }
</style>

</head>

    <body class="">
        <aside class="sidebar">
    <div class="container sidebar-sticky">
        <div class="light-dark" align="right">
    <button class="btn-light-dark">
        <svg class="moon" xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" viewBox="0 0 16 16">
            <path fill="currentColor" d="M6 .278a.768.768 0 0 1 .08.858a7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277c.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316a.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71C0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"/>
        </svg>
        <svg class="sun" xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" viewBox="0 0 16 16">
            <path fill="currentColor" d="M8 12a4 4 0 1 0 0-8a4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"/>
        </svg>
    </button>
</div>

        <div class="sidebar-about">
    <h1 class="brand">
        
        
            <a href="/">
                <h1>The Visible Net</h1>
            </a>
        
    </h1>
    <p class="lead">
    Understanding the mechanisms of machine intelligence
    </p>
</div>

        <nav>
    <ul class="sidebar-nav">

        
        
        
        
            
                
            
            
                
            
                
            
        
        
            
                
                    <li class="heading">
                        <a href="/posts/">Posts</a>
                    </li>
                    
                        <li class="sub-heading">
                            Recent
                        </li>
                        
                            <li class="bullet">
                                <a href="/posts/001-catching-up/">Catching Up</a>
                            </li>
                        
                            <li class="bullet">
                                <a href="/posts/000-welcome/">Welcome</a>
                            </li>
                        
                    
                
            
            
                
            
                
            
        

    </ul>
</nav>

        
    <a target="_blank" class="social" href="https://github.com">
        <svg xmlns="http://www.w3.org/2000/svg" width="1.2em" height="1.2em" viewBox="-2 -2 24 24">
            <path fill="currentColor" d="M18.88 1.099C18.147.366 17.265 0 16.233 0H3.746C2.714 0 1.832.366 1.099 1.099C.366 1.832 0 2.714 0 3.746v12.487c0 1.032.366 1.914 1.099 2.647c.733.733 1.615 1.099 2.647 1.099H6.66c.19 0 .333-.007.429-.02a.504.504 0 0 0 .286-.169c.095-.1.143-.245.143-.435l-.007-.885c-.004-.564-.006-1.01-.006-1.34l-.3.052c-.19.035-.43.05-.721.046a5.555 5.555 0 0 1-.904-.091a2.026 2.026 0 0 1-.872-.39a1.651 1.651 0 0 1-.572-.8l-.13-.3a3.25 3.25 0 0 0-.41-.663c-.186-.243-.375-.407-.566-.494l-.09-.065a.956.956 0 0 1-.17-.156a.723.723 0 0 1-.117-.182c-.026-.061-.004-.111.065-.15c.07-.04.195-.059.378-.059l.26.04c.173.034.388.138.643.311a2.1 2.1 0 0 1 .631.677c.2.355.44.626.722.813c.282.186.566.28.852.28c.286 0 .533-.022.742-.065a2.59 2.59 0 0 0 .585-.196c.078-.58.29-1.028.637-1.34a8.907 8.907 0 0 1-1.333-.234a5.314 5.314 0 0 1-1.223-.507a3.5 3.5 0 0 1-1.047-.872c-.277-.347-.505-.802-.683-1.365c-.177-.564-.266-1.215-.266-1.952c0-1.049.342-1.942 1.027-2.68c-.32-.788-.29-1.673.091-2.652c.252-.079.625-.02 1.119.175c.494.195.856.362 1.086.5c.23.14.414.257.553.352a9.233 9.233 0 0 1 2.497-.338c.859 0 1.691.113 2.498.338l.494-.312a6.997 6.997 0 0 1 1.197-.572c.46-.174.81-.221 1.054-.143c.39.98.424 1.864.103 2.653c.685.737 1.028 1.63 1.028 2.68c0 .737-.089 1.39-.267 1.957c-.177.568-.407 1.023-.689 1.366a3.65 3.65 0 0 1-1.053.865c-.42.234-.828.403-1.223.507a8.9 8.9 0 0 1-1.333.235c.45.39.676 1.005.676 1.846v3.11c0 .147.021.266.065.357a.36.36 0 0 0 .208.189c.096.034.18.056.254.064c.074.01.18.013.318.013h2.914c1.032 0 1.914-.366 2.647-1.099c.732-.732 1.099-1.615 1.099-2.647V3.746c0-1.032-.367-1.914-1.1-2.647z"/>
        </svg>
    </a>


    <a target="_blank" class="social" href="https://gitlab.com">
        <svg xmlns="http://www.w3.org/2000/svg" width="1.2em" height="1.2em" viewBox="-2 -2.5 24 24">
            <path fill="currentColor" d='M10.006 18.443L6.326 7.118h7.36l-3.68 11.325zm0 0L1.168 7.118h5.158l3.68 11.325zM1.168 7.118l8.838 11.325-9.68-7.032a.762.762 0 0 1-.276-.852l1.118-3.441zm0 0L3.385.296a.38.38 0 0 1 .724 0l2.217 6.822H1.168zm8.838 11.325l3.68-11.325h5.157l-8.837 11.325zm8.837-11.325l1.119 3.441a.762.762 0 0 1-.277.852l-9.68 7.032 8.838-11.325zm0 0h-5.157L15.902.296a.38.38 0 0 1 .725 0l2.216 6.822z' />

        </svg>
    </a>


    <a target="_blank" class="social" href="https://linkedin.com">
        <svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1.2em" viewBox="0 0 448 512">
            <path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5c0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7c-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5c67.2 0 79.7 44.3 79.7 101.9V416z"/>
        </svg>
    </a>


    <a target="_blank" class="social" href="https://twitter.com">
        <svg xmlns="http://www.w3.org/2000/svg" width="1.2em" height="1.2em" viewBox="0 0 16 16">
            <path fill="currentColor" d="M5.032 14.286c6.037 0 9.34-4.837 9.34-9.032c0-.137 0-.274-.01-.41A6.56 6.56 0 0 0 16 3.2c-.6.256-1.235.425-1.885.5a3.207 3.207 0 0 0 1.443-1.757c-.645.37-1.35.63-2.085.77a3.322 3.322 0 0 0-1.862-.958a3.384 3.384 0 0 0-2.082.334a3.223 3.223 0 0 0-1.442 1.49a3.08 3.08 0 0 0-.208 2.03a9.57 9.57 0 0 1-3.747-.963a9.269 9.269 0 0 1-3.018-2.354a3.086 3.086 0 0 0-.36 2.314c.189.787.68 1.475 1.376 1.924a3.344 3.344 0 0 1-1.49-.398v.04c0 .734.263 1.444.743 2.01a3.3 3.3 0 0 0 1.89 1.102c-.483.128-.99.146-1.482.055a3.19 3.19 0 0 0 1.168 1.577a3.36 3.36 0 0 0 1.9.627A6.732 6.732 0 0 1 0 12.86a9.527 9.527 0 0 0 5.032 1.423"/>
        </svg>
    </a>


    <a target="_blank" class="social" rel="me" href="https://mastodon.social">
        <svg xmlns="http://www.w3.org/2000/svg" width="1.2em" height="1.2em" viewBox="0 0 24 24">
            <path fill="currentColor" d="M20.94 14c-.28 1.41-2.44 2.96-4.97 3.26c-1.31.15-2.6.3-3.97.24c-2.25-.11-4-.54-4-.54v.62c.32 2.22 2.22 2.35 4.03 2.42c1.82.05 3.44-.46 3.44-.46l.08 1.65s-1.28.68-3.55.81c-1.25.07-2.81-.03-4.62-.5c-3.92-1.05-4.6-5.24-4.7-9.5l-.01-3.43c0-4.34 2.83-5.61 2.83-5.61C6.95 2.3 9.41 2 11.97 2h.06c2.56 0 5.02.3 6.47.96c0 0 2.83 1.27 2.83 5.61c0 0 .04 3.21-.39 5.43M18 8.91c0-1.08-.3-1.91-.85-2.56c-.56-.63-1.3-.96-2.23-.96c-1.06 0-1.87.41-2.42 1.23l-.5.88l-.5-.88c-.56-.82-1.36-1.23-2.43-1.23c-.92 0-1.66.33-2.23.96C6.29 7 6 7.83 6 8.91v5.26h2.1V9.06c0-1.06.45-1.62 1.36-1.62c1 0 1.5.65 1.5 1.93v2.79h2.07V9.37c0-1.28.5-1.93 1.51-1.93c.9 0 1.35.56 1.35 1.62v5.11H18V8.91Z"/>
        </svg>
    </a>


    <a target="_blank" class="social" href="https://tryhackme.com">
        <svg xmlns="http://www.w3.org/2000/svg" width="1.2em" height="1.2em" viewBox="0 0 24 24" >
            <path fill="white" d="M10.705 0C7.54 0 4.902 2.285 4.349 5.291a4.525 4.525 0 0 0-4.107 4.5 4.525 4.525 0 0 0 4.52 4.52h6.761a.625.625 0 1 0 0-1.25H4.761a3.273 3.273 0 0 1-3.27-3.27A3.273 3.273 0 0 1 6.59 7.08a.625.625 0 0 0 .7-1.035 4.488 4.488 0 0 0-1.68-.69 5.223 5.223 0 0 1 5.096-4.104 5.221 5.221 0 0 1 5.174 4.57 4.489 4.489 0 0 0-.488.305.625.625 0 1 0 .731 1.013 3.245 3.245 0 0 1 1.912-.616 3.278 3.278 0 0 1 3.203 2.61.625.625 0 0 0 1.225-.251 4.533 4.533 0 0 0-4.428-3.61 4.54 4.54 0 0 0-.958.105C16.556 2.328 13.9 0 10.705 0zm5.192 10.64a.925.925 0 0 0-.462.108.913.913 0 0 0-.313.29 1.27 1.27 0 0 0-.175.427 2.39 2.39 0 0 0-.054.514c0 .181.018.353.054.517.036.164.095.307.175.43a.899.899 0 0 0 .313.297c.127.073.281.11.462.11.18 0 .334-.037.46-.11a.897.897 0 0 0 .309-.296c.08-.124.137-.267.173-.431.036-.164.054-.336.054-.517 0-.18-.018-.352-.054-.514a1.271 1.271 0 0 0-.173-.426.901.901 0 0 0-.309-.291.917.917 0 0 0-.46-.108zm6.486 0a.925.925 0 0 0-.462.108.913.913 0 0 0-.313.29 1.27 1.27 0 0 0-.175.427 2.39 2.39 0 0 0-.053.514c0 .181.017.353.053.517.036.164.095.307.175.43a.899.899 0 0 0 .313.297c.127.073.281.11.462.11.18 0 .334-.037.46-.11a.897.897 0 0 0 .31-.296c.078-.124.136-.267.172-.431.036-.164.054-.336.054-.517 0-.18-.018-.352-.054-.514a1.271 1.271 0 0 0-.173-.426.901.901 0 0 0-.308-.291.916.916 0 0 0-.461-.108zm-8.537.068l-.84.618.313.43.476-.368v1.877h.603v-2.557zm6.486 0l-.841.618.314.43.477-.368v1.877h.603v-2.557zm-4.435.445c.08 0 .143.028.193.084.05.057.087.127.114.21.026.083.044.173.054.269a2.541 2.541 0 0 1 0 .533c-.01.097-.028.187-.054.27a.584.584 0 0 1-.114.21.243.243 0 0 1-.193.085.248.248 0 0 1-.195-.086.584.584 0 0 1-.118-.209 1.245 1.245 0 0 1-.056-.27 2.645 2.645 0 0 1 0-.533c.01-.096.029-.186.056-.27a.583.583 0 0 1 .118-.209.25.25 0 0 1 .195-.084zm6.486 0c.08 0 .144.028.193.084.05.057.087.127.114.21.027.083.044.173.054.269a2.541 2.541 0 0 1 0 .533c-.01.097-.027.187-.054.27a.584.584 0 0 1-.114.21.243.243 0 0 1-.193.085.249.249 0 0 1-.195-.086.581.581 0 0 1-.117-.209 1.245 1.245 0 0 1-.056-.27 2.642 2.642 0 0 1 0-.533c.01-.096.028-.186.056-.27a.58.58 0 0 1 .117-.209.25.25 0 0 1 .195-.084zm-2.191 3.51a.93.93 0 0 0-.463.109.908.908 0 0 0-.312.291c-.08.122-.139.263-.175.426a2.383 2.383 0 0 0-.054.514c0 .18.018.353.054.516.036.164.094.308.175.432a.91.91 0 0 0 .312.296.92.92 0 0 0 .463.11c.18 0 .333-.037.46-.11a.892.892 0 0 0 .308-.296 1.32 1.32 0 0 0 .174-.432c.036-.163.054-.335.054-.516 0-.18-.018-.352-.054-.514a1.274 1.274 0 0 0-.174-.426.89.89 0 0 0-.309-.291.918.918 0 0 0-.46-.108zm-6.402.07l-.841.617.314.43.476-.369v1.878h.604v-2.557zm2.125 0l-.841.617.314.43.477-.369v1.878h.603v-2.557zm2.116 0l-.84.617.313.43.477-.369v1.878h.603v-2.557zm2.16.443c.08 0 .144.028.194.085a.605.605 0 0 1 .114.21c.026.083.044.172.053.269a2.639 2.639 0 0 1 0 .532 1.28 1.28 0 0 1-.053.27.585.585 0 0 1-.114.21.244.244 0 0 1-.193.085.25.25 0 0 1-.196-.085.589.589 0 0 1-.117-.21 1.245 1.245 0 0 1-.056-.27 2.597 2.597 0 0 1 0-.532c.01-.097.028-.186.056-.27a.589.589 0 0 1 .117-.209.249.249 0 0 1 .196-.085zm-6.729 3.073a.676.676 0 0 0-.335.078.661.661 0 0 0-.227.211.91.91 0 0 0-.127.31c-.027.118-.04.242-.04.373s.013.256.04.375a.93.93 0 0 0 .127.313.65.65 0 0 0 .227.215c.092.053.204.08.335.08a.655.655 0 0 0 .334-.08.65.65 0 0 0 .225-.215c.057-.09.1-.194.125-.313a1.75 1.75 0 0 0 .04-.375c0-.13-.014-.255-.04-.373a.931.931 0 0 0-.125-.31.658.658 0 0 0-.225-.21.667.667 0 0 0-.334-.08zm3.086 0a.675.675 0 0 0-.336.078.661.661 0 0 0-.226.211.907.907 0 0 0-.127.31 1.69 1.69 0 0 0-.04.373c0 .131.013.256.04.375a.928.928 0 0 0 .127.313c.058.09.134.162.226.215.093.053.205.08.336.08a.655.655 0 0 0 .334-.08.65.65 0 0 0 .224-.215c.058-.09.1-.194.126-.313a1.752 1.752 0 0 0 0-.748.94.94 0 0 0-.126-.31.657.657 0 0 0-.224-.21.667.667 0 0 0-.334-.08zm5.108 0a.675.675 0 0 0-.336.078.661.661 0 0 0-.226.211.91.91 0 0 0-.127.31c-.027.118-.04.242-.04.373s.013.256.04.375a.931.931 0 0 0 .127.313c.058.09.134.162.226.215.093.053.205.08.336.08.13 0 .243-.027.334-.08a.65.65 0 0 0 .224-.215c.058-.09.1-.194.126-.313a1.75 1.75 0 0 0 .04-.375c0-.13-.014-.255-.04-.373a.943.943 0 0 0-.126-.31.657.657 0 0 0-.224-.21.668.668 0 0 0-.334-.08zm-6.658.05l-.61.448.227.311.346-.266v1.362h.438v-1.856zm3.068 0l-.61.448.227.311.346-.266v1.362h.438v-1.856zm5.108 0l-.611.448.228.311.346-.266v1.362h.438v-1.856zm-9.712.322c.058 0 .105.02.14.062a.421.421 0 0 1 .083.151.96.96 0 0 1 .04.196 1.932 1.932 0 0 1 0 .386.954.954 0 0 1-.04.197.421.421 0 0 1-.083.152.176.176 0 0 1-.14.061.18.18 0 0 1-.141-.06.427.427 0 0 1-.085-.153.887.887 0 0 1-.041-.197 1.96 1.96 0 0 1 0-.386.893.893 0 0 1 .04-.196.42.42 0 0 1 .086-.151.181.181 0 0 1 .141-.062zm3.086 0c.058 0 .104.02.14.062a.421.421 0 0 1 .082.151.94.94 0 0 1 .04.196 1.906 1.906 0 0 1 0 .386.93.93 0 0 1-.04.197.421.421 0 0 1-.082.152.176.176 0 0 1-.14.061.18.18 0 0 1-.141-.06.42.42 0 0 1-.086-.153.846.846 0 0 1-.04-.197 1.965 1.965 0 0 1-.011-.195c0-.057.004-.121.01-.191a.849.849 0 0 1 .041-.196.42.42 0 0 1 .086-.151.182.182 0 0 1 .141-.062zm5.108 0c.058 0 .104.02.14.062a.421.421 0 0 1 .082.151.92.92 0 0 1 .04.196 1.963 1.963 0 0 1 0 .386.943.943 0 0 1-.04.197.421.421 0 0 1-.082.152.177.177 0 0 1-.14.061.18.18 0 0 1-.142-.06.437.437 0 0 1-.085-.153.95.95 0 0 1-.04-.197 1.965 1.965 0 0 1-.011-.195c0-.057.004-.121.01-.191a.959.959 0 0 1 .04-.196.47.47 0 0 1 .086-.151.181.181 0 0 1 .142-.062zm-1.684 1.814a.675.675 0 0 0-.336.079.66.66 0 0 0-.227.21.91.91 0 0 0-.127.31 1.731 1.731 0 0 0 0 .748.939.939 0 0 0 .127.314c.059.09.134.162.227.215.093.053.205.08.336.08a.66.66 0 0 0 .334-.08.648.648 0 0 0 .224-.215c.058-.09.1-.195.126-.314a1.737 1.737 0 0 0-.001-.747.928.928 0 0 0-.125-.31.65.65 0 0 0-.224-.211.668.668 0 0 0-.334-.079zm3.063 0a.676.676 0 0 0-.336.079.664.664 0 0 0-.227.21.906.906 0 0 0-.127.31 1.74 1.74 0 0 0 0 .748.936.936 0 0 0 .127.314.66.66 0 0 0 .227.215c.092.053.204.08.336.08a.654.654 0 0 0 .334-.08.648.648 0 0 0 .223-.215c.058-.09.1-.195.126-.314a1.74 1.74 0 0 0 0-.747.928.928 0 0 0-.126-.31.65.65 0 0 0-.223-.211.666.666 0 0 0-.334-.079zm-1.545.05l-.611.448.228.312.346-.267v1.363h.438v-1.856zm-1.518.323c.057 0 .104.02.14.061a.42.42 0 0 1 .082.152.91.91 0 0 1 .04.195 1.966 1.966 0 0 1 0 .387.951.951 0 0 1-.04.197.421.421 0 0 1-.082.152.177.177 0 0 1-.14.06.18.18 0 0 1-.142-.06.428.428 0 0 1-.085-.152.914.914 0 0 1-.04-.197 1.96 1.96 0 0 1-.011-.195c0-.058.003-.122.01-.192a.923.923 0 0 1 .041-.195c.02-.06.048-.11.085-.152a.181.181 0 0 1 .142-.061zm3.063 0c.057 0 .104.02.14.061a.42.42 0 0 1 .082.152.94.94 0 0 1 .04.195 1.91 1.91 0 0 1 0 .387.93.93 0 0 1-.04.197.422.422 0 0 1-.083.152.175.175 0 0 1-.14.06.18.18 0 0 1-.141-.06.423.423 0 0 1-.085-.152.907.907 0 0 1-.04-.197 1.95 1.95 0 0 1 0-.387.915.915 0 0 1 .04-.195c.02-.06.048-.11.085-.152a.182.182 0 0 1 .142-.061zm-9.713.185a.465.465 0 0 0-.232.055.456.456 0 0 0-.157.146.627.627 0 0 0-.089.215 1.168 1.168 0 0 0-.027.259c0 .09.009.177.027.26a.648.648 0 0 0 .089.216c.04.063.093.112.157.149a.459.459 0 0 0 .232.056c.09 0 .168-.02.231-.056a.45.45 0 0 0 .156-.149.67.67 0 0 0 .087-.217 1.218 1.218 0 0 0 0-.518.647.647 0 0 0-.087-.215.448.448 0 0 0-.156-.146.458.458 0 0 0-.23-.055zm1.052.035l-.423.31.158.217.24-.185v.944h.303v-1.286zm-1.052.224c.04 0 .073.014.097.042a.284.284 0 0 1 .057.105.69.69 0 0 1 .028.136c.004.049.007.092.007.133 0 .04-.003.086-.007.135a.684.684 0 0 1-.028.136.285.285 0 0 1-.057.105.123.123 0 0 1-.097.043.125.125 0 0 1-.098-.043.298.298 0 0 1-.059-.105.612.612 0 0 1-.028-.136 1.39 1.39 0 0 1 0-.268.62.62 0 0 1 .028-.136.297.297 0 0 1 .06-.105.125.125 0 0 1 .097-.042zm3.775 1.394a.463.463 0 0 0-.232.054.452.452 0 0 0-.157.146.621.621 0 0 0-.088.214 1.19 1.19 0 0 0 0 .519.641.641 0 0 0 .088.217.46.46 0 0 0 .157.15.458.458 0 0 0 .232.054.454.454 0 0 0 .232-.055.45.45 0 0 0 .155-.149.664.664 0 0 0 .087-.217 1.189 1.189 0 0 0 0-.519.642.642 0 0 0-.087-.214.446.446 0 0 0-.155-.146.459.459 0 0 0-.232-.054zm1.052.034l-.423.31.158.216.24-.185v.945h.303V22.68zm-1.052.223c.04 0 .073.014.098.043a.3.3 0 0 1 .057.105.643.643 0 0 1 .027.135 1.31 1.31 0 0 1 0 .268.654.654 0 0 1-.027.137.307.307 0 0 1-.057.105.124.124 0 0 1-.098.042.125.125 0 0 1-.098-.042.293.293 0 0 1-.059-.105.618.618 0 0 1-.028-.137 1.364 1.364 0 0 1 0-.268.612.612 0 0 1 .028-.135.287.287 0 0 1 .06-.105.123.123 0 0 1 .097-.043z"></path>
        </svg>
    </a>


    <a target="_blank" class="social" href="https://discord.com">
        <svg xmlns="http://www.w3.org/2000/svg" width="1.2em" height="1.2em" viewBox="0 0 24 24">
            <path fill="currentColor" d="M19.27 5.33C17.94 4.71 16.5 4.26 15 4a.09.09 0 0 0-.07.03c-.18.33-.39.76-.53 1.09a16.09 16.09 0 0 0-4.8 0c-.14-.34-.35-.76-.54-1.09c-.01-.02-.04-.03-.07-.03c-1.5.26-2.93.71-4.27 1.33c-.01 0-.02.01-.03.02c-2.72 4.07-3.47 8.03-3.1 11.95c0 .02.01.04.03.05c1.8 1.32 3.53 2.12 5.24 2.65c.03.01.06 0 .07-.02c.4-.55.76-1.13 1.07-1.74c.02-.04 0-.08-.04-.09c-.57-.22-1.11-.48-1.64-.78c-.04-.02-.04-.08-.01-.11c.11-.08.22-.17.33-.25c.02-.02.05-.02.07-.01c3.44 1.57 7.15 1.57 10.55 0c.02-.01.05-.01.07.01c.11.09.22.17.33.26c.04.03.04.09-.01.11c-.52.31-1.07.56-1.64.78c-.04.01-.05.06-.04.09c.32.61.68 1.19 1.07 1.74c.03.01.06.02.09.01c1.72-.53 3.45-1.33 5.25-2.65c.02-.01.03-.03.03-.05c.44-4.53-.73-8.46-3.1-11.95c-.01-.01-.02-.02-.04-.02zM8.52 14.91c-1.03 0-1.89-.95-1.89-2.12s.84-2.12 1.89-2.12c1.06 0 1.9.96 1.89 2.12c0 1.17-.84 2.12-1.89 2.12zm6.97 0c-1.03 0-1.89-.95-1.89-2.12s.84-2.12 1.89-2.12c1.06 0 1.9.96 1.89 2.12c0 1.17-.83 2.12-1.89 2.12z"/>
        </svg>
    </a>


    <a target="_blank" class="social" href="https://youtube.com">
        <svg xmlns="http://www.w3.org/2000/svg" width="1.2em" height="1.2em" viewBox="0 0 24 24">
            <path fill="currentColor" d="M12.006 19.012h-.02c-.062 0-6.265-.012-7.83-.437a2.5 2.5 0 0 1-1.764-1.765A26.494 26.494 0 0 1 1.986 12a26.646 26.646 0 0 1 .417-4.817A2.564 2.564 0 0 1 4.169 5.4c1.522-.4 7.554-.4 7.81-.4H12c.063 0 6.282.012 7.831.437c.859.233 1.53.904 1.762 1.763c.29 1.594.427 3.211.407 4.831a26.568 26.568 0 0 1-.418 4.811a2.51 2.51 0 0 1-1.767 1.763c-1.52.403-7.553.407-7.809.407Zm-2-10.007l-.005 6l5.212-3l-5.207-3Z"/>
        </svg>
    </a>


    <a target="_blank" class="social" href="https://instagram.com">
        <svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1.2em" viewBox="0 0 448 512">
            <path fill="currentColor" d="M224.1 141c-63.6 0-114.9 51.3-114.9 114.9s51.3 114.9 114.9 114.9S339 319.5 339 255.9S287.7 141 224.1 141zm0 189.6c-41.1 0-74.7-33.5-74.7-74.7s33.5-74.7 74.7-74.7s74.7 33.5 74.7 74.7s-33.6 74.7-74.7 74.7zm146.4-194.3c0 14.9-12 26.8-26.8 26.8c-14.9 0-26.8-12-26.8-26.8s12-26.8 26.8-26.8s26.8 12 26.8 26.8zm76.1 27.2c-1.7-35.9-9.9-67.7-36.2-93.9c-26.2-26.2-58-34.4-93.9-36.2c-37-2.1-147.9-2.1-184.9 0c-35.8 1.7-67.6 9.9-93.9 36.1s-34.4 58-36.2 93.9c-2.1 37-2.1 147.9 0 184.9c1.7 35.9 9.9 67.7 36.2 93.9s58 34.4 93.9 36.2c37 2.1 147.9 2.1 184.9 0c35.9-1.7 67.7-9.9 93.9-36.2c26.2-26.2 34.4-58 36.2-93.9c2.1-37 2.1-147.8 0-184.8zM398.8 388c-7.8 19.6-22.9 34.7-42.6 42.6c-29.5 11.7-99.5 9-132.1 9s-102.7 2.6-132.1-9c-19.6-7.8-34.7-22.9-42.6-42.6c-11.7-29.5-9-99.5-9-132.1s-2.6-102.7 9-132.1c7.8-19.6 22.9-34.7 42.6-42.6c29.5-11.7 99.5-9 132.1-9s102.7-2.6 132.1 9c19.6 7.8 34.7 22.9 42.6 42.6c11.7 29.5 9 99.5 9 132.1s2.7 102.7-9 132.1z"/>
        </svg>
    </a>


    <a target="_blank" class="social" href="https://facebook.com">
        <svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1.2em" viewBox="0 0 448 512">
            <path fill="currentColor" d="M400 32H48A48 48 0 0 0 0 80v352a48 48 0 0 0 48 48h137.25V327.69h-63V256h63v-54.64c0-62.15 37-96.48 93.67-96.48c27.14 0 55.52 4.84 55.52 4.84v61h-31.27c-30.81 0-40.42 19.12-40.42 38.73V256h68.78l-11 71.69h-57.78V480H400a48 48 0 0 0 48-48V80a48 48 0 0 0-48-48z"/>
        </svg>
    </a>




        <p class="footnote">
powered by <a target="_blank" href="https://gohugo.io">Hugo</a> | themed with <a target="_blank" href="https://github.com/lukeorth/poison">poison</a>
    <br>
    &copy; 2023 . All rights reserved.
</p>

  </div>
</aside>

        <main class="content container">
            <div class="post">
  <div class="info">
    <h1 class="post-title">
        <a href="/posts/001-catching-up/">Catching Up</a>
    </h1>
    <time datetime="2023-03-26T14:36:12-0400" class="post-date">
        March 26, 2023
    </time>
    
    
    
    
    
    
</div>

  <p>Today, I received an email from my good college friend David Maymudes. David got his math degree from Harvard a few years ahead of me, and we have both worked at Microsoft and Google at overlapping times. He is still at Google now. We have both witnessed and helped drive major cycles of platform innovation in the industry in the past, and David is well aware of the important pieces of work that go into building a new technology ecosystem. From inside Google today, he is a direct witness to the transformation of that company as the profound new approaches to artificial intelligence become a corporate priority. It is obvious that something major is afoot: a new ecosystem is being created. Although David does not directly work on large-scale machine learning, it touches on his work, because it is touching everybody.</p>
<p>Despite being an outsider to our field, David reached out to ask some clarifying questions about some specific technical ideas, including <a href="https://huggingface.co/blog/rlhf">RLHF</a>, <a href="https://www.youtube.com/watch?v=EUjc1WuyPT8">AI safety</a>, and the new <a href="https://openai.com/blog/chatgpt-plugins">ChatGPT plug-in model</a>.  There is so much to catch up on.  In response to David&rsquo;s questions, I wrote up a crash-course in modern large language modeling, which we will delve into in this blog.</p>
<p>David&rsquo;s first question was whether people control models by literally training the whole thing as a single black box.  Or are there many boxes?</p>
<h1 id="yes-it-is-a-monolith">Yes, it is a monolith.</h1>
<p>To implement a base autoregressive (next-word-prediction) transformer model like GPT, we create a function that takes a sequence of words as input and produces the predicted next word as output. The output is actually a ranking of all the possible next words and their correct probability weights.</p>
<p>This function is implemented using a series of numerical steps. We turn each word fragment in the input into a vector of a few thousand numbers using a big lookup table, so that the whole sequence of input words will be many thousands of numbers. We then execute a single, crazy big high-dimensional calculation that applies a few rounds of simple arithmetic to all these numbers in a few dozen parallel steps, with each step transforming each array of many thousands of numbers to another array of just as many numbers.</p>
<p><img src="/images/gpt_arch_figure.png" alt="Autoregressive Transformer Architecture"></p>
<p>At each step, every single number could in principle depend on every number of the previous step, but the exact dependencies are determined by an architecture that does lookups in big parameter tables. The size of those tables puts a ceiling on the complexity that can be encoded. For example, GPT-3 has 175 billion parameters, while the size of GPT-4 is unknown but estimated to be about a trillion parameters. You can think of these sizes as &ldquo;the size of the interpreter that is processing the text.&rdquo; After the dozens of rounds of calculation, the final output vector is read off the last layer of the output, and that vector is compared to a final output lookup table (that contains one vector for each possible output word fragment) that is used to determine the possible next words.</p>
<p>This word-prediction-as-vector-calculation idea is a very simple framework that was developed in the 1990s by <a href="https://papers.baulab.info/Elman-1990.pdf">Jeff Ellman</a>, <a href="https://papers.baulab.info/also/Jordan-1986.pdf">Michael Jordan</a>, <a href="https://papers.baulab.info/Hochreiter-1997.pdf">Sepp Hochreiter</a>, <a href="https://papers.baulab.info/Bengio-2003.pdf">Yoshua Bengio</a>, and others. What&rsquo;s new in recent years is that we have found a good way to cleverly constrain the computations to impose some structure that reduce the complexity of the computation and improve parallelism in a very effective way. The structures that we impose today are &ldquo;attention heads&rdquo; (<a href="https://papers.baulab.info/Bahdanau-2015.pdf">a 2015 idea from Dzmitry Bahdanau</a>) and &ldquo;multilayer perceptrons&rdquo; (<a href="https://papers.baulab.info/Rosenblatt-1962.pdf">credit Rosenblatt from the 1960s</a>) and &ldquo;residual streams&rdquo; (<a href="https://papers.baulab.info/He-2015.pdf">due to Kaiming He in 2015</a>) and the way the pieces are put together is done is what people call the &ldquo;transformer architecture&rdquo; (<a href="https://papers.baulab.info/Vaswani-2017.pdf">devised by Ashish Vaswani from Google in 2017</a>).  All the gory details of transformers are explained beautifully by <a href="https://jalammar.github.io/illustrated-transformer/">Jay Alammmar in his illustrated transformer series</a>.</p>
<p>Transformers can be thought of as encouraging sparse data-associative wiring diagrams within all the calculations, and they have proven very effective at learning computational patterns that people might call intelligent. However, the computational paths that are learned within the transformer architecture form a crazy amalgamation of internal circuits that is opaque to us. We train it as a monolith to solve the very simple &ldquo;pretext task&rdquo; of predicting the probabilities of the possible next words as accurately as possible. The internal computational structure that emerges is very surprising, and that is the subject of the study of mechanistic interpretability.</p>
<h1 id="prompt-engineering-and-in-context-learning">Prompt engineering and in-context learning.</h1>
<p>The idea that so many capabilities could be learned from this task is very surprising and was not obvious to experts. Credit for some intuition goes to <a href="https://papers.baulab.info/Radford-2018.pdf">Alec Radford at OpenAI who pushed hard on scaling up autoregregressive transformers in the initial GPT work in 2018</a>.</p>
<p>In a series of increasing investments in larger GPT models trained with more parameters on more text, Radford and his collaborators found that this simple architecture was a remarkable chameleon. For example, it quickly guesses the context, and if it thinks it is in the middle of a book of poems, it will use a strategy that leads it to generate more poetry. But if it sees context that looks like a blog, it will generate blog-like text. The same applies for a spreadsheet, screenplay, FORTRAN program, Reddit thread, parallel multilingual text, answer key for a test, output of a computer script, dialogue between two people, diary describing the internal ruminations of a person, and so on. The extreme context-sensitivity seems like an oddball phenomenon, but it may be a linchpin that leads to reasonable models of cognition, as <a href="https://arxiv.org/abs/2212.01681">Jacob Andreas convincingly argued</a>.</p>
<p><img src="/images/prompt_style_figure.png" alt="Prompt engineering techniques"></p>
<p>The observation of context-sensitivity has led to a flood of <a href="https://arxiv.org/abs/2107.13586">&ldquo;prompt engineering&rdquo;</a> that demonstrates all the tricks you can do by setting up some clever input context for the model. For example, if you set up the input text to look like a standardized test, you can see how well the model can complete the answer key, and what is remarkable is that the large models contain <a href="https://arxiv.org/abs/1909.01066">real-world knowledge of facts and relationships</a>, or even the ability to <a href="https://arxiv.org/abs/2103.03874">solve some math problems</a>. With different prompts, it is stunning to see the model do things like <a href="https://arxiv.org/abs/2302.09210">translate between French and English</a> when you prompt it as if it is working on parallel text, predict the output of a computer program given the code, write out step-by-step reasoning for some complex problem, or even follow instructions, e.g. &ldquo;write me a poem,&rdquo; and then it predicts that the best next words are a poem.</p>
<p>One of the most effective ways to create a prompt is called <a href="https://arxiv.org/abs/2301.00234">&ldquo;in-context learning&rdquo;</a> where you give a handful of fully-worked examples the way that you&rsquo;d like to see them done (starting with the problem and followed by the answer), and then you put the problem as the last example and ask the model to make its prediction.  This works even better if you do it in a way that writes out the work explicitly; that is called <a href="https://arxiv.org/abs/2201.11903">&ldquo;chain of thought&rdquo; prompting</a>).</p>
<p>However, it is more convenient for people when a model is able to solve a problem &ldquo;zero shot,&rdquo; i.e., with no worked examples shown at all. Some consensus is now forming that the most useful mode for these models is the &ldquo;instruction-following&rdquo; mode, where instead of requiring a few examples, you give it an explicit instruction like &ldquo;write me a poem,&rdquo; and then it follows with a poem.  Base pretrained GPT can do that to some extent, but it is often unsure about whether it should actually follow the instructions or not. Perhaps after you give it an instruction, it cannot decide whether it should follow some other pattern, such as creating a list of other possible different instructions.</p>
<h1 id="instruction-fine-tuning-and-ai-safety">Instruction fine-tuning and AI safety.</h1>
<p>One of the main techniques in modern machine learning is <a href="https://papers.baulab.info/Yosinksi-2014.pdf">transfer learning</a>, which involves starting with a pre-trained model&rsquo;s capabilities and then fine-tuning its parameters to better fit the data from the specific context where it will be applied. The goal is to focus the model&rsquo;s capabilities on the problem at hand. For example, the original GPT model can write fallacious, fractured, or even fanatical text because there is a lot of such text in real-world training data. However, we might want to fine-tune the model to mimic an honest, humanlike, and humane style.</p>
<p>To achieve this, teams at <a href="https://arxiv.org/abs/2203.02155">OpenAI</a>, <a href="https://arxiv.org/pdf/2109.01652.pdf">Google</a>, <a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">Stanford</a>, and others have collected thousands of examples of instruction-following dialogues that have the attributes required. For instance, when given an instruction in the input, the model should predict a response that gives a direct, correct, well-reasoned, well-written, and ethical answer to that instruction.</p>
<p>How is such fine-tuning done? OpenAI fine-tunes GPT on instruction-following using a combination of direct word-prediction training and reinforcement learning based on a learned policy model. However, the use of reinforcement learning is an implementation detail that not everyone uses, and it is reasonable to simply tune the model to predict words in the new data set. The basic idea is to adjust the model so that it begins with the assumption that it is working in the particular instruction-following linguistic context that we want it to be in.</p>
<p>The usefulness and power of that linguistic context depend on three things: (1) the pretraining data (all the world&rsquo;s text); (2) the pretraining architecture (what computations the transformer was able to learn from the world&rsquo;s text); and (3) the specific form of the instruction feedback fine-tuning (in particular the instruction-tuning example data).</p>
<p>For example, if you fine-tune a model on text written at a 3rd grade level, you will expect it to produce 3rd grade output. But if you fine-tune it on an amazing range of difficult and useful instruction-following tasks, then you can hope it will mimic that ability to solve difficult problems. You can look at examples of instruction-following data within the <a href="https://huggingface.co/datasets/tatsu-lab/alpaca">Stanford Alpaca dataset, here on huggingface</a>.</p>
<p>This ability to fine-tune a model to follow our explicit instructions also touches on concerns that people like <a href="https://www.youtube.com/watch?v=EUjc1WuyPT8">Eliezer Yudowsky</a> have long had that large-scale machine learning may end up producing systems that end up being destructive to humanity.  Instrutions seem to give us a way to make models a little safer.
We can train models to follow our instructions and to follow societal norms and even ethical constraints by giving lots of examples. <a href="https://arxiv.org/abs/2212.08073">Anthropic</a>, in particular, has been thinking about how to encode ethical reasoning into model fine-tuning data.</p>
<p>Why this might work at all is itself wondrous (and again, the subject of study in this blog). If we are genuinely interested in AI safety, it demands a deeper understanding of what is happening under the hood. But training models to follow our instructions by-example seems to work. So that is the modern program.</p>
<h1 id="retrieval-methods-tool-use-and-secrecy">Retrieval methods, tool use, and secrecy.</h1>
<p>It is obvious that there is a fourth thing you might want the model to be able to depend on, which is tools and resources in the real world.</p>
<p>So you can make this fourth thing part of the fine-tuning data.  For example, you could instruct the model, so that, as part of answering, when it would be approprate to do a Google search for X, it should output a special token <code>[invoke googlesearch X]</code>, and then it should expect to see further input that includes all the top Google search results.  Or when it would be appropriate to see the contents of a web page at url U, it should say <code>[invoke webget U]</code> and it would be able to see the contents of the web page on the input. Of course an ordinary GPT model might have no idea when it should be doing web searches and so on, but perhaps if we give it a few examples, or better yet we you include many thousand fully-worked interactions of this form, where we pantomime the interaction with the open web within an instruction-fine-tuning data set, then it will start to exploit this new form of interaction.</p>
<p>We could bake such interactions into the transformer during pre-training; these are called &ldquo;retrieval based architectures,&rdquo; (e.g., see <a href="https://arxiv.org/abs/2112.04426">RETRO, Sebastian Borgeaud 2021</a>), and they work.  They tend to create models that do not waste all their parameters memorizing the world&rsquo;s knowledge, but instead they will make requests to see different pieces of text when making predictions. But you could also add such interactions at fine-tuning time, after pretraining on ordinary word prediction.  Because fine-tuning is fast and much more amenable to quick experimentation and engineering than training from scratch, that is probably what I would expect to see in practice.  What you would need is a data set of thousands of worked examples of tool use.</p>
<p><img src="/images/tool_use_figure.png" alt="Tool use by a language model"></p>
<p>OpenAI has not revealed the internal architecture or training methods used to create ChatGPT, but three days ago they released a new plugin architecture that suggests that tool-use has been a major part of their work on ChatGPT.  You can see some examples of their plug-in architecture here.</p>
<p><a href="https://platform.openai.com/docs/plugins/examples">https://platform.openai.com/docs/plugins/examples</a>.</p>
<p>OpenAI has not disclosed how they created this plugin architecture, but I imagine that what they have done is to create a general-purpose template that they have included in their instruction-following fine-tuning procedure where the model learns to recognize some form of <code>[invoke tool with input X]</code> for making a request to an external tool and seeing the response.  They have probably created a large dataset of such tool use, maybe by instrumenting web browsers to watch people using lots of tools online including search engines, calendars, online stores, etc.  Or maybe by having a team of people making REST API calls for successful interactions and encoding these as examples &ldquo;instruction following&rdquo; scripts.</p>
<p>So, in this world, how does the system decide which one is relevant to use?</p>
<p>OpenAI has not disclosed how they are training ChatGPT to make this choice, but this could be solved in a number of ways, for example, in their instruction-fine-tuning examples, they could provide a piece of text to the input of GPT that lists all the descriptions of tools, and then after that context, they could have examples of choosing the right tool from the list at an appropriate moment during conversation, while being careful to demosntragte in the training data that tools that is not on the listed input should never be used.  After seeing a few thousand of such worked examples during fine-tuning, I would expect it to be able to mimic generic tool use pretty well.</p>
<p>We are in an interesting but concerning new era now, where the key decision-making of the algorithm is doubly opaque.  First, the way that OpenAI has hooked together their system is opaque to us, since it is a proprietary trade secret.  That is similar to how Google&rsquo;s search ranking is opaque to outsiders, since Google will not reveal its very clever internal methods for deciding between websites.</p>
<p>But now, there is a second, profound issue, that should drive our real concern.  Unlike the case of Google (you and I both are aware of Google&rsquo;s internal query debugging facilities), when working with a massive model like GPT-3 or GPT-4, the decision-making of the model is opauqe even to OpenAI themselves.  When the model chooses one tool over another, the engineers may not have any insight as to &ldquo;why,&rdquo; beyond the billions of tangled arithmetic operations that led to the prediction of a token.</p>
<p>We will train our models and guide our models using billions of pretraining examples, cleverly chosen architectures, and thousands of fine-tuning examples.  But then the actual algorithm that it applies is the result of a massive and opaque optimization.</p>
<p>What are these systems actually learning?</p>
<p>That is the most urgent problem facing computer science, and maybe all of science, today.</p>
<p>It is important to ask the question transparently and openly, and not as a trade secret.</p>
<p>That&rsquo;s why I left Google to work on this.</p>
<p>Join us!</p>

  <hr>
<div class="footer">
    
	    
            
	            <a class="previous-post" href="/posts/000-welcome/?ref=footer">« Welcome</a>
	        
        
	    
    
</div>

  <div class="article-toc " >
    <h4></h4>
    <nav id="TableOfContents"></nav>
</div>

</div>
        </main>
    </body>
</html>
